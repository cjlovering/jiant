{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "\n",
    "import pyinflect\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"gap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = {\n",
    "    \"subj\": [\n",
    "        \"we\",\n",
    "        \"they\",\n",
    "        \"he\",\n",
    "        \"she\",\n",
    "        \"you\"\n",
    "    ],\n",
    "    \"prefix_verb\": [\n",
    "        \"know\"\n",
    "    ],\n",
    "    \"verb\": [\n",
    "     'acknowledge',\n",
    "     'believe',\n",
    "     'determine',\n",
    "     'discover',\n",
    "     'hold',\n",
    "     'know',\n",
    "     'mention',\n",
    "     'notice',\n",
    "     'observe',\n",
    "     'recognize',\n",
    "     'recommend',\n",
    "     'remember',\n",
    "     'require',\n",
    "     'reveal',\n",
    "     'show',\n",
    "     'suspect',\n",
    "     'understand',\n",
    "     'love'\n",
    "    ],\n",
    "    \"object\": [\n",
    "        \"someone\",\n",
    "        \"everyone\",\n",
    "        \"them\",\n",
    "        \"her\",\n",
    "        \"him\",\n",
    "        \"ourselves\",\n",
    "        \"myself\"\n",
    "    ],\n",
    "    \"continuation\": [\n",
    "        \"by the deadline\",\n",
    "        \"last semester\",\n",
    "        \"last year\",\n",
    "        \"last week\",\n",
    "        \"in the middle of the night\",\n",
    "        \"after the shocking incident\",\n",
    "        \"over the summer\",\n",
    "        \"over the past decade\",\n",
    "        \"during the financial crisis\",\n",
    "        \"last semester\",\n",
    "        \"last week\",\n",
    "        \"last winter\",\n",
    "        \"earlier that week\",\n",
    "        \"last month\",\n",
    "        \"before the trial\"\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"en_core_web_lg\"\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "def get_parenthetical():\n",
    "    s, v = inflect(\"who\", random.choice(verbs))\n",
    "    out = [s, v, random.choice(data[\"object\"])]\n",
    "    return \" \".join(out)\n",
    "\n",
    "def inflect(noun, verb):\n",
    "    sent = \" \".join([noun, verb])\n",
    "    doc = nlp(sent)\n",
    "    inflection = doc[1].tag_ if doc[1].tag_ in ['VBD', 'VB', 'VBG'] else 'VBD'\n",
    "    vi = doc[1]._.inflect(inflection)\n",
    "    if vi is None:\n",
    "        return noun, verb\n",
    "    else:\n",
    "        return noun, vi\n",
    "\n",
    "def i_me(sent):\n",
    "    words = set(sent.split())\n",
    "    if \"I\" in words and \"me\" in words:\n",
    "        return sent.replace(\"me\", \"myself\")\n",
    "    return sent\n",
    "\n",
    "def we_us(sent):\n",
    "    words = set(sent.split())\n",
    "    if \"we\" in words and \"us\" in words:\n",
    "        return sent.replace(\"us\", \"ourselves\")\n",
    "    return sent\n",
    "\n",
    "def fix(sent):\n",
    "    sent = i_me(sent)\n",
    "    sent = we_us(sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def stringify(sent):\n",
    "    sent = \" \".join(sent).replace(' ,', ',')\n",
    "    sent = fix(sent)\n",
    "    sent = sent[0].upper() + sent[1:]\n",
    "    return sent \n",
    "\n",
    "def complement(prev_subjs, prev_verbs):\n",
    "    subjs = [s for s in data['subj'] if s not in prev_subjs]\n",
    "    subj = random.choice(subjs)\n",
    "    \n",
    "    verbs = [v for v in data['verb'] if v not in prev_verbs]\n",
    "    verb = random.choice(verbs)\n",
    "    \n",
    "    return inflect(subj, verb)\n",
    "\n",
    "def get_parts(N, words, splice_obj = False):\n",
    "    prefix_subj = \"I\"  # random.choice(data['subj'])\n",
    "    prefix_verb = random.choice(data['prefix_verb'])\n",
    "\n",
    "    if splice_obj:\n",
    "        splice_obj = random.choice(data['object']) # [cp_2_verb]\n",
    "        embeds, parenthetical_count = get_embeds_splice_obj(N, words, splice_obj)\n",
    "    else:\n",
    "        embeds, parenthetical_count = get_embeds(N, words)\n",
    "\n",
    "    obj = random.choice(data['object']) # [cp_2_verb]\n",
    "\n",
    "    continuation = random.choice(data['continuation'])\n",
    "    info = {\n",
    "        'parenthetical_count': parenthetical_count,\n",
    "        'clause_count': N\n",
    "    }\n",
    "    return prefix_subj, prefix_verb, embeds, obj, continuation, info\n",
    "\n",
    "def get_embeds(N, words):\n",
    "    embeds = []\n",
    "    P = 1 / (N * 2)\n",
    "    parenthetical_count = 0\n",
    "    for i in range(N):\n",
    "        if i < N:\n",
    "            embeds.append(words[i])\n",
    "        s, v = complement([], [])\n",
    "        if random.random() < P and parenthetical_count == 0:\n",
    "            parenthetical = get_parenthetical()\n",
    "            embeds.extend([s, parenthetical, v])\n",
    "            parenthetical_count += 1\n",
    "        else:\n",
    "            embeds.extend([s, v])\n",
    "    return embeds, parenthetical_count\n",
    "\n",
    "def get_embeds_splice_obj(N, words, obj):\n",
    "    embeds = []\n",
    "    P = 1 / (N * 2)\n",
    "    parenthetical_count = 0\n",
    "    # For instance, if N is 2, then its 0. If N is 3, then its 1 or 2.\n",
    "    if N == 2:\n",
    "        splice_level = 0\n",
    "        words = [\"who\", \"that\"]\n",
    "        \n",
    "    elif N == 3:\n",
    "        if random.random() < 0.67:\n",
    "            splice_level = 1\n",
    "            words = random.choice([\n",
    "               [\"who\", \"that\", \"that\"],\n",
    "               [\"that\", \"who\", \"that\"]\n",
    "            ])\n",
    "        else:\n",
    "            splice_level = 0\n",
    "            words = [\"who\", \"that\", \"that\"]\n",
    "    else:\n",
    "        assert False, f\"Expected N <= 3, but N = {N}, MAX = {MAX}.\"\n",
    "    for i in range(N):\n",
    "        if i < N:\n",
    "            embeds.append(words[i])\n",
    "        s, v = complement([], [])\n",
    "        if random.random() < P and parenthetical_count == 0:\n",
    "            parenthetical = get_parenthetical()\n",
    "            embeds.extend([s, parenthetical, v])\n",
    "            parenthetical_count += 1\n",
    "        else:\n",
    "            embeds.extend([s, v])\n",
    "        if splice_level == i:\n",
    "            embeds.append(obj)\n",
    "    return embeds, parenthetical_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>section</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>template</th>\n",
       "      <th>parenthetical_count</th>\n",
       "      <th>clause_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10541</th>\n",
       "      <td>I know who we recognized someone that they who noticed myself loved over the summer</td>\n",
       "      <td>bad-only</td>\n",
       "      <td>no</td>\n",
       "      <td>S_wh_gap_obj</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5299</th>\n",
       "      <td>I know that he knew that you who revealed everyone recognized who we knew her last winter</td>\n",
       "      <td>neither</td>\n",
       "      <td>no</td>\n",
       "      <td>S_wh_no_gap</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9571</th>\n",
       "      <td>I know that you recognized over the summer</td>\n",
       "      <td>neither</td>\n",
       "      <td>no</td>\n",
       "      <td>S_that_gap</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5202</th>\n",
       "      <td>I know that she observed that she who knew him loved who you understood myself over the past decade</td>\n",
       "      <td>neither</td>\n",
       "      <td>no</td>\n",
       "      <td>S_wh_no_gap</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9346</th>\n",
       "      <td>I know that they observed over the past decade</td>\n",
       "      <td>neither</td>\n",
       "      <td>no</td>\n",
       "      <td>S_that_gap</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097</th>\n",
       "      <td>I know that he knew that we who recommended her noticed who she observed myself last semester</td>\n",
       "      <td>neither</td>\n",
       "      <td>no</td>\n",
       "      <td>S_wh_no_gap</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10366</th>\n",
       "      <td>I know that he acknowledged who you suspected someone that we acknowledged before the trial</td>\n",
       "      <td>bad-only</td>\n",
       "      <td>no</td>\n",
       "      <td>S_wh_gap_obj</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10420</th>\n",
       "      <td>I know who they recommended myself that we knew that he determined last week</td>\n",
       "      <td>bad-only</td>\n",
       "      <td>no</td>\n",
       "      <td>S_wh_gap_obj</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>I know that she held who they understood that he acknowledged last semester</td>\n",
       "      <td>both</td>\n",
       "      <td>yes</td>\n",
       "      <td>S_wh_gap</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>I know that we who mentioned myself observed someone last semester</td>\n",
       "      <td>both</td>\n",
       "      <td>yes</td>\n",
       "      <td>S_that_no_gap</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  sentence  \\\n",
       "10541                  I know who we recognized someone that they who noticed myself loved over the summer   \n",
       "5299             I know that he knew that you who revealed everyone recognized who we knew her last winter   \n",
       "9571                                                            I know that you recognized over the summer   \n",
       "5202   I know that she observed that she who knew him loved who you understood myself over the past decade   \n",
       "9346                                                        I know that they observed over the past decade   \n",
       "7097         I know that he knew that we who recommended her noticed who she observed myself last semester   \n",
       "10366          I know that he acknowledged who you suspected someone that we acknowledged before the trial   \n",
       "10420                         I know who they recommended myself that we knew that he determined last week   \n",
       "1913                           I know that she held who they understood that he acknowledged last semester   \n",
       "3094                                    I know that we who mentioned myself observed someone last semester   \n",
       "\n",
       "        section acceptable       template  parenthetical_count  clause_count  \n",
       "10541  bad-only         no   S_wh_gap_obj                    1             2  \n",
       "5299    neither         no    S_wh_no_gap                    1             3  \n",
       "9571    neither         no     S_that_gap                    0             1  \n",
       "5202    neither         no    S_wh_no_gap                    1             3  \n",
       "9346    neither         no     S_that_gap                    0             1  \n",
       "7097    neither         no    S_wh_no_gap                    1             3  \n",
       "10366  bad-only         no   S_wh_gap_obj                    0             3  \n",
       "10420  bad-only         no   S_wh_gap_obj                    0             3  \n",
       "1913       both        yes       S_wh_gap                    0             3  \n",
       "3094       both        yes  S_that_no_gap                    1             1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX = 3\n",
    "\n",
    "def S_wh_gap():\n",
    "    N = random.randint(1, MAX)\n",
    "    words = [\"that\"] * (N - 1) + [\"who\"]\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [continuation], info\n",
    "\n",
    "def S_that_no_gap():\n",
    "    N = random.randint(1, MAX)\n",
    "    words = [\"that\"] * (N)\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [obj, continuation], info\n",
    "\n",
    "def S_wh_no_gap():\n",
    "    N = random.randint(1, MAX)\n",
    "    words = [\"that\"] * (N - 1) + [\"who\"]\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [obj, continuation], info\n",
    "\n",
    "def S_that_gap():\n",
    "    N = random.randint(1, MAX)\n",
    "    words = [\"that\"] * (N)\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [continuation], info\n",
    "\n",
    "def S_wh_gap_obj():\n",
    "    # NOTE: This setup doesn't work with only one clause -- it folds into `S_wh_no_gap`.\n",
    "    N = random.randint(1 + 1, MAX)\n",
    "    words = [\"that\"] * (N - 1) + [\"who\"]\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words, splice_obj = True)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [continuation], info\n",
    "\n",
    "\n",
    "filler_templates = [  \n",
    "    ('S_wh_gap', 'both', 'yes', S_wh_gap),\n",
    "    ('S_that_no_gap', 'both', 'yes', S_that_no_gap),\n",
    "    ('S_wh_no_gap', 'neither', 'no', S_wh_no_gap),\n",
    "    ('S_that_gap', 'neither', 'no', S_that_gap),\n",
    "    ('S_wh_gap_obj', 'bad-only', 'no', S_wh_gap_obj),\n",
    "]\n",
    "\n",
    "count = 2500\n",
    "output = []\n",
    "\n",
    "for name, section, acceptable, template in filler_templates:\n",
    "    for _ in range(count):\n",
    "        parts, info = template()\n",
    "        sent = stringify(parts)\n",
    "        output.append({\n",
    "            **{\n",
    "            \"sentence\": sent,\n",
    "            \"section\": section,\n",
    "            \"acceptable\": acceptable,\n",
    "            \"template\": name\n",
    "            }, \n",
    "             **info,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(output)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = df.sort_values([\"acceptable\", \"section\", \"template\", \"parenthetical_count\", \"clause_count\"])\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cjlovering/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates('sentence')\n",
    "df[\"label\"] = (df.acceptable == \"yes\").astype(int)\n",
    "df.to_csv(f\"filler-gap-{count}.tsv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = ['S_wh_gap', 'S_that_no_gap', 'S_wh_no_gap', 'S_that_gap']\n",
    "bad_only = ['S_wh_gap_obj']\n",
    "\n",
    "SPLIT_SIZE = 1000\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "from sklearn.model_selection import train_test_split\n",
    "for t in templates:\n",
    "    x = df[df.template == t]\n",
    "    _train, _test = train_test_split(x, test_size=0.5)\n",
    "    train.append(_train.sample(SPLIT_SIZE))\n",
    "    test.append(_test.sample(SPLIT_SIZE))\n",
    "    \n",
    "train_df = pd.concat(train)\n",
    "test_df = pd.concat(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SIZE = len(train_df) \n",
    "\n",
    "SIZE_ORIG_1, SIZE_NEW_1 = round(TOTAL_SIZE * 0.99), round(TOTAL_SIZE * 0.01)\n",
    "SIZE_ORIG_5, SIZE_NEW_5 = round(TOTAL_SIZE * 0.99), round(TOTAL_SIZE * 0.01)\n",
    "\n",
    "# train_bad = \n",
    "\n",
    "t = 'S_wh_gap_obj'\n",
    "x = df[df.template == t]\n",
    "train_bad, test_bad = train_test_split(x, test_size=0.5)\n",
    "train_bad, test_bad = train_bad.sample(SPLIT_SIZE), test_bad.sample(SPLIT_SIZE)\n",
    "\n",
    "all_train = pd.concat([train_df, train_bad])\n",
    "test = pd.concat([test_df, test_bad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_jsonl(df, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(df.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both / weak ! [weak]\n",
    "_weak_both_train = all_train[all_train.section == 'both'].sample(1000)\n",
    "_weak_weak_train = all_train[all_train.section == 'bad-only']\n",
    "_weak_both_test = test[test.section == 'both'].sample(1000)\n",
    "_weak_weak_test = test[test.section == 'bad-only']\n",
    "\n",
    "_weak_probing_train = pd.concat([_weak_both_train, _weak_weak_train])\n",
    "_weak_probing_test = pd.concat([_weak_both_test, _weak_weak_test])\n",
    "\n",
    "to_jsonl(_weak_probing_train, f\"{FOLDER}/gap_probing_weak_train.jsonl\")\n",
    "to_jsonl(_weak_probing_test, f\"{FOLDER}/gap_probing_weak_val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both / neither ! [strong]\n",
    "_strong_both_train = all_train[all_train.section == 'both'].sample(1000)\n",
    "_strong_neither_train = all_train[all_train.section == 'neither'].sample(1000)\n",
    "_strong_both_test = test[test.section == 'both'].sample(1000)\n",
    "_strong_neither_test = test[test.section == 'neither'].sample(1000)\n",
    "\n",
    "_strong_probing_train = pd.concat([_strong_both_train, _strong_neither_train])\n",
    "_strong_probing_test = pd.concat([_strong_both_test, _strong_neither_test])\n",
    "\n",
    "to_jsonl(_strong_probing_train, f\"{FOLDER}/gap_probing_strong_train.jsonl\")\n",
    "to_jsonl(_strong_probing_test, f\"{FOLDER}/gap_probing_strong_val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_strong_both_train = all_train[all_train.section == 'both']\n",
    "_strong_neither_train = all_train[all_train.section == 'neither']\n",
    "_strong_both_test = test[test.section == 'both']\n",
    "_strong_neither_test = test[test.section == 'neither']\n",
    "\n",
    "_strong_probing_train = pd.concat([_strong_both_train, _strong_neither_train])\n",
    "_strong_probing_test = pd.concat([_strong_both_test, _strong_neither_test])\n",
    "\n",
    "to_jsonl(_strong_probing_train, f\"{FOLDER}/gap_finetune_0_train.jsonl\")\n",
    "to_jsonl(_strong_probing_test, f\"{FOLDER}/gap_finetune_0_val.jsonl\")\n",
    "\n",
    "gap_finetune_1_train = pd.concat([_strong_probing_train.sample(SIZE_ORIG_1), train_bad.sample(SIZE_NEW_1)])\n",
    "gap_finetune_1_val = pd.concat([_strong_probing_test.sample(SIZE_ORIG_1), test_bad.sample(SIZE_NEW_1)])\n",
    "\n",
    "to_jsonl(gap_finetune_1_train, f\"{FOLDER}/gap_finetune_1_train.jsonl\")\n",
    "to_jsonl(gap_finetune_1_val, f\"{FOLDER}/gap_finetune_1_val.jsonl\")\n",
    "\n",
    "gap_finetune_5_train = pd.concat([_strong_probing_train.sample(SIZE_ORIG_5), train_bad.sample(SIZE_NEW_5)])\n",
    "gap_finetune_5_val = pd.concat([_strong_probing_test.sample(SIZE_ORIG_5), test_bad.sample(SIZE_NEW_5)])\n",
    "\n",
    "to_jsonl(gap_finetune_5_train, f\"{FOLDER}/gap_finetune_5_train.jsonl\")\n",
    "to_jsonl(gap_finetune_5_val, f\"{FOLDER}/gap_finetune_5_val.jsonl\")\n",
    "\n",
    "to_jsonl(test, f\"{FOLDER}/gap_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weak / neither ?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-torch",
   "language": "python",
   "name": "conda-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
