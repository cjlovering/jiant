{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "\n",
    "import pyinflect\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"isl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = [\n",
    "     'acknowledge',\n",
    "     'believe',\n",
    "     'determine',\n",
    "     'discover',\n",
    "     'hold',\n",
    "     'know',\n",
    "     'mention',\n",
    "     'notice',\n",
    "     'observe',\n",
    "     'recognize',\n",
    "     'recommend',\n",
    "     'remember',\n",
    "     'require',\n",
    "     'reveal',\n",
    "     'show',\n",
    "     'suspect',\n",
    "     'understand',\n",
    "     'love'\n",
    "    ]\n",
    "data  = {\n",
    "    \"subj\": [\n",
    "        \"we\",\n",
    "        \"they\",\n",
    "        \"he\",\n",
    "        \"she\",\n",
    "        \"you\"\n",
    "    ],\n",
    "    \"prefix_verb\": [\n",
    "        \"know\",\n",
    "        \"think\",\n",
    "        \"wonder\",\n",
    "        \"hope\"\n",
    "    ],\n",
    "    \"verb\": verbs,\n",
    "    \"object\": [\n",
    "        \"someone\",\n",
    "        \"everyone\",\n",
    "        \"them\",\n",
    "        \"her\",\n",
    "        \"him\",\n",
    "        \"ourselves\",\n",
    "        \"myself\"\n",
    "    ],\n",
    "    \"continuation\": [\n",
    "        \"by the deadline\",\n",
    "        \"last semester\",\n",
    "        \"last year\",\n",
    "        \"last week\",\n",
    "        \"in the middle of the night\",\n",
    "        \"after the shocking incident\",\n",
    "        \"over the summer\",\n",
    "        \"over the past decade\",\n",
    "        \"during the financial crisis\",\n",
    "        \"last semester\",\n",
    "        \"last week\",\n",
    "        \"last winter\",\n",
    "        \"earlier that week\",\n",
    "        \"last month\",\n",
    "        \"before the trial\"\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"en_core_web_lg\"\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "def get_parenthetical():\n",
    "    s, v = inflect(\"who\", random.choice(verbs))\n",
    "    out = [s, v, random.choice(data[\"object\"])]\n",
    "    return \" \".join(out)\n",
    "\n",
    "def inflect(noun, verb):\n",
    "    sent = \" \".join([noun, verb])\n",
    "    doc = nlp(sent)\n",
    "    inflection = doc[1].tag_ if doc[1].tag_ in ['VBD', 'VB', 'VBG'] else 'VBD'\n",
    "    vi = doc[1]._.inflect(inflection)\n",
    "    if vi is None:\n",
    "        return noun, verb\n",
    "    else:\n",
    "        return noun, vi\n",
    "\n",
    "def i_me(sent):\n",
    "    words = set(sent.split())\n",
    "    if \"I\" in words and \"me\" in words:\n",
    "        return sent.replace(\"me\", \"myself\")\n",
    "    return sent\n",
    "\n",
    "def we_us(sent):\n",
    "    words = set(sent.split())\n",
    "    if \"we\" in words and \"us\" in words:\n",
    "        return sent.replace(\"us\", \"ourselves\")\n",
    "    return sent\n",
    "\n",
    "def fix(sent):\n",
    "    sent = i_me(sent)\n",
    "    sent = we_us(sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def stringify(sent):\n",
    "    sent = \" \".join(sent).replace(' ,', ',')\n",
    "    sent = fix(sent)\n",
    "    sent = sent[0].upper() + sent[1:]\n",
    "    return sent \n",
    "\n",
    "def complement(prev_subjs, prev_verbs):\n",
    "    subjs = [s for s in data['subj'] if s not in prev_subjs]\n",
    "    subj = random.choice(subjs)\n",
    "    \n",
    "    verbs = [v for v in data['verb'] if v not in prev_verbs]\n",
    "    verb = random.choice(verbs)\n",
    "    \n",
    "    return inflect(subj, verb)\n",
    "\n",
    "def get_parts(N, words, splice_obj = False):\n",
    "    prefix_subj = \"I\"  # random.choice(data['subj'])\n",
    "    prefix_verb = random.choice(data['prefix_verb'])\n",
    "\n",
    "    if splice_obj:\n",
    "        splice_obj = random.choice(data['object']) # [cp_2_verb]\n",
    "        embeds, parenthetical_count = get_embeds_splice_obj(N, words, splice_obj)\n",
    "    else:\n",
    "        embeds, parenthetical_count = get_embeds(N, words)\n",
    "\n",
    "    obj = random.choice(data['object']) # [cp_2_verb]\n",
    "\n",
    "    continuation = random.choice(data['continuation'])\n",
    "    info = {\n",
    "        'parenthetical_count': parenthetical_count,\n",
    "        'clause_count': N\n",
    "    }\n",
    "    return prefix_subj, prefix_verb, embeds, obj, continuation, info\n",
    "\n",
    "def get_embeds(N, words):\n",
    "    embeds = []\n",
    "    P = 1 / (N * 2)\n",
    "    parenthetical_count = 0\n",
    "    for i in range(N):\n",
    "        if i < N:\n",
    "            embeds.append(words[i])\n",
    "        s, v = complement([], [])\n",
    "        if random.random() < P and parenthetical_count == 0:\n",
    "            parenthetical = get_parenthetical()\n",
    "            embeds.extend([s, parenthetical, v])\n",
    "            parenthetical_count += 1\n",
    "        else:\n",
    "            embeds.extend([s, v])\n",
    "    return embeds, parenthetical_count\n",
    "\n",
    "def get_embeds_splice_obj(N, words, obj):\n",
    "    embeds = []\n",
    "    P = 1 / (N * 2)\n",
    "    parenthetical_count = 0\n",
    "    # For instance, if N is 2, then its 0. If N is 3, then its 1 or 2.\n",
    "    if N == 2:\n",
    "        splice_level = 0\n",
    "        words = [\"who\", \"that\"]\n",
    "        \n",
    "    elif N == 3:\n",
    "        if random.random() < 0.67:\n",
    "            splice_level = 1\n",
    "            words = random.choice([\n",
    "               [\"who\", \"that\", \"that\"],\n",
    "               [\"that\", \"who\", \"that\"]\n",
    "            ])\n",
    "        else:\n",
    "            splice_level = 0\n",
    "            words = [\"who\", \"that\", \"that\"]\n",
    "    else:\n",
    "        assert False, f\"Expected N <= 3, but N = {N}, MAX = {MAX}.\"\n",
    "    for i in range(N):\n",
    "        if i < N:\n",
    "            embeds.append(words[i])\n",
    "        s, v = complement([], [])\n",
    "        if random.random() < P and parenthetical_count == 0:\n",
    "            parenthetical = get_parenthetical()\n",
    "            embeds.extend([s, parenthetical, v])\n",
    "            parenthetical_count += 1\n",
    "        else:\n",
    "            embeds.extend([s, v])\n",
    "        if splice_level == i:\n",
    "            embeds.append(obj)\n",
    "    return embeds, parenthetical_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>section</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>template</th>\n",
       "      <th>parenthetical_count</th>\n",
       "      <th>clause_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>I wonder that she suspected that he recognized who they discovered last winter</td>\n",
       "      <td>both</td>\n",
       "      <td>yes</td>\n",
       "      <td>S_wh_gap</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>I wonder who he determined last semester</td>\n",
       "      <td>both</td>\n",
       "      <td>yes</td>\n",
       "      <td>S_wh_gap</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>I think who he revealed last semester</td>\n",
       "      <td>both</td>\n",
       "      <td>yes</td>\n",
       "      <td>S_wh_gap</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11055</th>\n",
       "      <td>I wonder that you recommended that she discovered that you suspected last winter</td>\n",
       "      <td>neither</td>\n",
       "      <td>no</td>\n",
       "      <td>S_that_gap</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3560</th>\n",
       "      <td>I hope that you acknowledged that they determined him last semester</td>\n",
       "      <td>both</td>\n",
       "      <td>yes</td>\n",
       "      <td>S_that_no_gap</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>I think that she loved who she observed that we showed in the middle of the night</td>\n",
       "      <td>both</td>\n",
       "      <td>yes</td>\n",
       "      <td>S_wh_gap</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10109</th>\n",
       "      <td>I think that they who knew ourselves loved that you discovered that they determined after the shocking incident</td>\n",
       "      <td>neither</td>\n",
       "      <td>no</td>\n",
       "      <td>S_that_gap</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8597</th>\n",
       "      <td>I think that she discovered who we determined that we who remembered him discovered ourselves over the summer</td>\n",
       "      <td>neither</td>\n",
       "      <td>no</td>\n",
       "      <td>S_wh_no_gap</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>I know who you who revealed everyone determined that we believed last year</td>\n",
       "      <td>both</td>\n",
       "      <td>yes</td>\n",
       "      <td>S_wh_gap</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7829</th>\n",
       "      <td>I wonder who you determined that they acknowledged someone by the deadline</td>\n",
       "      <td>neither</td>\n",
       "      <td>no</td>\n",
       "      <td>S_wh_no_gap</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              sentence  \\\n",
       "1541                                    I wonder that she suspected that he recognized who they discovered last winter   \n",
       "456                                                                           I wonder who he determined last semester   \n",
       "2045                                                                             I think who he revealed last semester   \n",
       "11055                                 I wonder that you recommended that she discovered that you suspected last winter   \n",
       "3560                                               I hope that you acknowledged that they determined him last semester   \n",
       "2243                                 I think that she loved who she observed that we showed in the middle of the night   \n",
       "10109  I think that they who knew ourselves loved that you discovered that they determined after the shocking incident   \n",
       "8597     I think that she discovered who we determined that we who remembered him discovered ourselves over the summer   \n",
       "902                                         I know who you who revealed everyone determined that we believed last year   \n",
       "7829                                        I wonder who you determined that they acknowledged someone by the deadline   \n",
       "\n",
       "       section acceptable       template  parenthetical_count  clause_count  \n",
       "1541      both        yes       S_wh_gap                    0             3  \n",
       "456       both        yes       S_wh_gap                    0             1  \n",
       "2045      both        yes       S_wh_gap                    0             1  \n",
       "11055  neither         no     S_that_gap                    0             3  \n",
       "3560      both        yes  S_that_no_gap                    0             2  \n",
       "2243      both        yes       S_wh_gap                    0             3  \n",
       "10109  neither         no     S_that_gap                    1             3  \n",
       "8597   neither         no    S_wh_no_gap                    1             3  \n",
       "902       both        yes       S_wh_gap                    1             2  \n",
       "7829   neither         no    S_wh_no_gap                    0             2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX = 3\n",
    "\n",
    "def S_wh_gap():\n",
    "    N = random.randint(1, MAX)\n",
    "    words = [\"that\"] * (N - 1) + [\"who\"]\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [continuation], info\n",
    "\n",
    "def S_that_no_gap():\n",
    "    N = random.randint(1, MAX)\n",
    "    words = [\"that\"] * (N)\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [obj, continuation], info\n",
    "\n",
    "def S_wh_no_gap():\n",
    "    N = random.randint(1, MAX)\n",
    "    words = [\"that\"] * (N - 1) + [\"who\"]\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [obj, continuation], info\n",
    "\n",
    "def S_that_gap():\n",
    "    N = random.randint(1, MAX)\n",
    "    words = [\"that\"] * (N)\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [continuation], info\n",
    "\n",
    "def S_wh_gap_obj():\n",
    "    # NOTE: This setup doesn't work with only one clause -- it folds into `S_wh_no_gap`.\n",
    "    N = random.randint(1 + 1, MAX)\n",
    "    words = [\"that\"] * (N - 1) + [\"who\"]\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words, splice_obj = True)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [continuation], info\n",
    "\n",
    "\n",
    "def S_wh_wh_gap():\n",
    "    N = random.randint(2, MAX)\n",
    "    words = [\"that\"] * (N - 2) + [\"who\", \"who\"]\n",
    "    random.shuffle(words)\n",
    "    prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "    return [prefix_subj, prefix_verb] + embeds + [continuation], info\n",
    "\n",
    "\n",
    "filler_templates = [  \n",
    " ('S_wh_gap', 'both', 'yes', S_wh_gap),\n",
    "    ('S_that_no_gap', 'both', 'yes', S_that_no_gap),\n",
    "        ('S_wh_wh_gap', 'bad-only', 'no', S_wh_wh_gap),\n",
    "    ('S_wh_no_gap', 'neither', 'no', S_wh_no_gap),\n",
    "    ('S_that_gap', 'neither', 'no', S_that_gap),\n",
    "#     ('S_wh_gap_obj', 'bad-only', 'no', S_wh_gap_obj),\n",
    "]\n",
    "\n",
    "\n",
    "count = 2500\n",
    "output = []\n",
    "\n",
    "for name, section, acceptable, template in filler_templates:\n",
    "    for _ in range(count):\n",
    "        parts, info = template()\n",
    "        sent = stringify(parts)\n",
    "        output.append({\n",
    "            **{\n",
    "            \"sentence\": sent,\n",
    "            \"section\": section,\n",
    "            \"acceptable\": acceptable,\n",
    "            \"template\": name\n",
    "            }, \n",
    "             **info,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(output)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = df.sort_values([\"acceptable\", \"section\", \"template\", \"parenthetical_count\", \"clause_count\"])\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates('sentence')\n",
    "df[\"label\"] = (df.acceptable == \"yes\").astype(int)\n",
    "df.to_csv(f\"isl-{count}.tsv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = ['S_wh_gap', 'S_that_no_gap', 'S_wh_no_gap', 'S_that_gap']\n",
    "bad_only = ['S_wh_wh_gap']\n",
    "\n",
    "SPLIT_SIZE = 1000\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "from sklearn.model_selection import train_test_split\n",
    "for t in templates:\n",
    "    x = df[df.template == t]\n",
    "    _train, _test = train_test_split(x, test_size=0.5)\n",
    "    train.append(_train.sample(SPLIT_SIZE))\n",
    "    test.append(_test.sample(SPLIT_SIZE))\n",
    "    \n",
    "train_df = pd.concat(train)\n",
    "test_df = pd.concat(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SIZE = len(train_df) \n",
    "\n",
    "SIZE_ORIG_1, SIZE_NEW_1 = round(TOTAL_SIZE * 0.99), round(TOTAL_SIZE * 0.01)\n",
    "SIZE_ORIG_5, SIZE_NEW_5 = round(TOTAL_SIZE * 0.99), round(TOTAL_SIZE * 0.01)\n",
    "\n",
    "# train_bad = \n",
    "\n",
    "t = 'S_wh_wh_gap'\n",
    "x = df[df.template == t]\n",
    "train_bad, test_bad = train_test_split(x, test_size=0.5)\n",
    "train_bad, test_bad = train_bad.sample(SPLIT_SIZE), test_bad.sample(SPLIT_SIZE)\n",
    "\n",
    "all_train = pd.concat([train_df, train_bad])\n",
    "test = pd.concat([test_df, test_bad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_jsonl(df, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(df.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both / weak ! [weak]\n",
    "_weak_both_train = all_train[all_train.section == 'both'].sample(1000)\n",
    "_weak_weak_train = all_train[all_train.section == 'bad-only']\n",
    "_weak_both_test = test[test.section == 'both'].sample(1000)\n",
    "_weak_weak_test = test[test.section == 'bad-only']\n",
    "\n",
    "_weak_probing_train = pd.concat([_weak_both_train, _weak_weak_train])\n",
    "_weak_probing_test = pd.concat([_weak_both_test, _weak_weak_test])\n",
    "\n",
    "to_jsonl(_weak_probing_train, f\"{FOLDER}/isl_probing_weak_train.jsonl\")\n",
    "to_jsonl(_weak_probing_test, f\"{FOLDER}/isl_probing_weak_val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both / neither ! [strong]\n",
    "_strong_both_train = all_train[all_train.section == 'both'].sample(1000)\n",
    "_strong_neither_train = all_train[all_train.section == 'neither'].sample(1000)\n",
    "_strong_both_test = test[test.section == 'both'].sample(1000)\n",
    "_strong_neither_test = test[test.section == 'neither'].sample(1000)\n",
    "\n",
    "_strong_probing_train = pd.concat([_strong_both_train, _strong_neither_train])\n",
    "_strong_probing_test = pd.concat([_strong_both_test, _strong_neither_test])\n",
    "\n",
    "to_jsonl(_strong_probing_train, f\"{FOLDER}/isl_probing_strong_train.jsonl\")\n",
    "to_jsonl(_strong_probing_test, f\"{FOLDER}/isl_probing_strong_val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_strong_both_train = all_train[all_train.section == 'both']\n",
    "_strong_neither_train = all_train[all_train.section == 'neither']\n",
    "_strong_both_test = test[test.section == 'both']\n",
    "_strong_neither_test = test[test.section == 'neither']\n",
    "\n",
    "_strong_probing_train = pd.concat([_strong_both_train, _strong_neither_train])\n",
    "_strong_probing_test = pd.concat([_strong_both_test, _strong_neither_test])\n",
    "\n",
    "to_jsonl(_strong_probing_train, f\"{FOLDER}/isl_finetune_0_train.jsonl\")\n",
    "to_jsonl(_strong_probing_test, f\"{FOLDER}/isl_finetune_0_val.jsonl\")\n",
    "\n",
    "gap_finetune_1_train = pd.concat([_strong_probing_train.sample(SIZE_ORIG_1), train_bad.sample(SIZE_NEW_1)])\n",
    "gap_finetune_1_val = pd.concat([_strong_probing_test.sample(SIZE_ORIG_1), test_bad.sample(SIZE_NEW_1)])\n",
    "\n",
    "to_jsonl(gap_finetune_1_train, f\"{FOLDER}/isl_finetune_1_train.jsonl\")\n",
    "to_jsonl(gap_finetune_1_val, f\"{FOLDER}/isl_finetune_1_val.jsonl\")\n",
    "\n",
    "gap_finetune_5_train = pd.concat([_strong_probing_train.sample(SIZE_ORIG_5), train_bad.sample(SIZE_NEW_5)])\n",
    "gap_finetune_5_val = pd.concat([_strong_probing_test.sample(SIZE_ORIG_5), test_bad.sample(SIZE_NEW_5)])\n",
    "\n",
    "to_jsonl(gap_finetune_5_train, f\"{FOLDER}/isl_finetune_5_train.jsonl\")\n",
    "to_jsonl(gap_finetune_5_val, f\"{FOLDER}/isl_finetune_5_val.jsonl\")\n",
    "\n",
    "to_jsonl(test, f\"{FOLDER}/isl_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weak / neither ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def S_wh_gap():\n",
    "#     N = random.randint(1, MAX)\n",
    "#     words = [\"that\"] * (N - 1) + [\"who\"]\n",
    "#     random.shuffle(words)\n",
    "#     prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "#     return [prefix_subj, prefix_verb] + embeds + [continuation], info\n",
    "\n",
    "# def S_that_no_gap():\n",
    "#     N = random.randint(1, MAX)\n",
    "#     words = [\"that\"] * (N)\n",
    "#     random.shuffle(words)\n",
    "#     prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "#     return [prefix_subj, prefix_verb] + embeds + [obj, continuation], info\n",
    "\n",
    "# def S_wh_no_gap():\n",
    "#     N = random.randint(1, MAX)\n",
    "#     words = [\"that\"] * (N - 1) + [\"who\"]\n",
    "#     random.shuffle(words)\n",
    "#     prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "#     return [prefix_subj, prefix_verb] + embeds + [obj, continuation], info\n",
    "\n",
    "# def S_that_gap():\n",
    "#     N = random.randint(1, MAX)\n",
    "#     words = [\"that\"] * (N)\n",
    "#     random.shuffle(words)\n",
    "#     prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words)\n",
    "#     return [prefix_subj, prefix_verb] + embeds + [continuation], info\n",
    "\n",
    "# def S_wh_gap_obj():\n",
    "#     # NOTE: This setup doesn't work with only one clause -- it folds into `S_wh_no_gap`.\n",
    "#     N = random.randint(1 + 1, MAX)\n",
    "#     words = [\"that\"] * (N - 1) + [\"who\"]\n",
    "#     random.shuffle(words)\n",
    "#     prefix_subj, prefix_verb, embeds, obj, continuation, info = get_parts(N, words, splice_obj = True)\n",
    "#     return [prefix_subj, prefix_verb] + embeds + [continuation], info\n",
    "\n",
    "\n",
    "# filler_templates = [  \n",
    "#     ('S_wh_gap', 'both', 'yes', S_wh_gap),\n",
    "#     ('S_that_no_gap', 'both', 'yes', S_that_no_gap),\n",
    "#     ('S_wh_no_gap', 'neither', 'no', S_wh_no_gap),\n",
    "#     ('S_that_gap', 'neither', 'no', S_that_gap),\n",
    "#     ('S_wh_gap_obj', 'bad-only', 'no', S_wh_gap_obj),\n",
    "# ]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-torch",
   "language": "python",
   "name": "conda-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
