{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>labels</th>\n",
       "      <th>preds</th>\n",
       "      <th>label</th>\n",
       "      <th>case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>He made no remark , but the matter remained in...</td>\n",
       "      <td>the matter remained in his thoughts , for he s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>entailment</td>\n",
       "      <td>c: a S clause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>He made no remark , but the matter remained in...</td>\n",
       "      <td>he stood in front of the fire afterwards with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>entailment</td>\n",
       "      <td>c: a S clause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>No woman would ever send a reply-paid telegram .</td>\n",
       "      <td>No woman would ever send a reply paid telegram .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>entailment</td>\n",
       "      <td>c: a S clause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>Well , sir , it did not appear to be a matter...</td>\n",
       "      <td>you have heard the facts</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>entailment</td>\n",
       "      <td>c: a S clause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>Well , sir , it did not appear to be a matter...</td>\n",
       "      <td>Well , sir , it did not appear to be a matter ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>entailment</td>\n",
       "      <td>c: a S clause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            premise  \\\n",
       "0           8  He made no remark , but the matter remained in...   \n",
       "1          12  He made no remark , but the matter remained in...   \n",
       "2          20   No woman would ever send a reply-paid telegram .   \n",
       "3          56   Well , sir , it did not appear to be a matter...   \n",
       "4          64   Well , sir , it did not appear to be a matter...   \n",
       "\n",
       "                                          hypothesis  labels  preds  \\\n",
       "0  the matter remained in his thoughts , for he s...       1      1   \n",
       "1  he stood in front of the fire afterwards with ...       1      1   \n",
       "2   No woman would ever send a reply paid telegram .       1      1   \n",
       "3                           you have heard the facts       1      1   \n",
       "4  Well , sir , it did not appear to be a matter ...       1      1   \n",
       "\n",
       "        label           case  \n",
       "0  entailment  c: a S clause  \n",
       "1  entailment  c: a S clause  \n",
       "2  entailment  c: a S clause  \n",
       "3  entailment  c: a S clause  \n",
       "4  entailment  c: a S clause  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('merged.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'but'\n",
    "\n",
    "Does the presence of 'but' throw off the model? *It isn't particularly over-represented in the premises, but it is in the hypotheses. If the sentence has 'but' in both the premise and the the hypothesis, it's likely to be misclassified. Definitely might be worth exploring further.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entailment, entailment: 182 in premise, 9 in hypothesis, 437 total\n",
      "entailment, contradiction: 12 in premise, 0 in hypothesis, 29 total\n",
      "contradiction, entailment: 84 in premise, 54 in hypothesis, 339 total\n",
      "contradiction, contradiction: 174 in premise, 48 in hypothesis, 980 total\n"
     ]
    }
   ],
   "source": [
    "num_to_label = {1:\"entailment\", 2:\"contradiction\"}\n",
    "\n",
    "for label in [1, 2]:\n",
    "    for pred in [1, 2]:\n",
    "        df_label = df[df.labels == label]\n",
    "        df_label_pred = df_label[df.preds == pred]\n",
    "        \n",
    "        premise_has_but = 0; hypothesis_has_but = 0\n",
    "        for _, row in df_label_pred.iterrows():\n",
    "            if 'but' in row.premise:\n",
    "                premise_has_but += 1\n",
    "            if 'but' in row.hypothesis:\n",
    "                hypothesis_has_but += 1\n",
    "        \n",
    "        print(\"{}, {}: {} in premise, {} in hypothesis, {} total\".format(\n",
    "            num_to_label[label], num_to_label[pred], premise_has_but, hypothesis_has_but, len(df_label_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length\n",
    "\n",
    "Does something about the length of the sentences matter? *It does seem that the pairs the model gets wrong are significantly longer in both the premise and hypothesis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entailment, entailment: 187.38443935926773 in premise, 49.31578947368421 in hypothesis\n",
      "entailment, contradiction: 121.03448275862068 in premise, 30.79310344827586 in hypothesis\n",
      "contradiction, entailment: 203.83480825958702 in premise, 89.84070796460178 in hypothesis\n",
      "contradiction, contradiction: 145.98673469387754 in premise, 59.38367346938775 in hypothesis\n"
     ]
    }
   ],
   "source": [
    "for label in [1, 2]:\n",
    "    for pred in [1, 2]:\n",
    "        df_label = df[df.labels == label]\n",
    "        df_label_pred = df_label[df.preds == pred]\n",
    "        \n",
    "        premise_length_sum = 0; hypothesis_length_sum = 0\n",
    "        for _, row in df_label_pred.iterrows():\n",
    "            premise_length_sum += len(row.premise)\n",
    "            hypothesis_length_sum += len(row.hypothesis)\n",
    "        \n",
    "        print(\"{}, {}: {} in premise, {} in hypothesis\".format(\n",
    "            num_to_label[label], num_to_label[pred], premise_length_sum / len(df_label_pred), hypothesis_length_sum / len(df_label_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each cue word\n",
    "As a sanity check, we'll check the error rate for each cue word. *It seems that nor leads to often incorrect predictions, though this might be because of badly formed sentences.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "no :\n",
      "entailment: 10 / 141 incorrect\n",
      "contradiction: 94 / 358 incorrect\n",
      "\n",
      "not :\n",
      "entailment: 18 / 322 incorrect\n",
      "contradiction: 241 / 921 incorrect\n",
      "\n",
      "never :\n",
      "entailment: 2 / 45 incorrect\n",
      "contradiction: 38 / 128 incorrect\n",
      "\n",
      "nor :\n",
      "entailment: 2 / 15 incorrect\n",
      "contradiction: 19 / 31 incorrect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "for cue_word in ['no ', 'not ', 'never ', 'nor ']:\n",
    "    print(\"\\n{}:\".format(cue_word))\n",
    "    df_filtered = df[df.premise.str.contains(cue_word)]\n",
    "\n",
    "    for label in [1, 2]:\n",
    "        incorrect_label = 2 if label == 1 else 1\n",
    "        \n",
    "        df_label = df_filtered[df.labels == label]\n",
    "        total = len(df_label)\n",
    "        num_incorrect = len(df_label[df.preds == incorrect_label])\n",
    "        print(\"{}: {} / {} incorrect\".format(\n",
    "            num_to_label[label], num_incorrect, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For different numbers of cue words\n",
    "I'm wondering if more cue words means more errors. *Nothing very striking is popping out. It doesn't look like 2 cue words is much more likely to lead to mis-classification than 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 cue words:\n",
      "entailment: 1 / 7 incorrect\n",
      "contradiction: 9 / 16 incorrect\n",
      "\n",
      "1 cue words:\n",
      "entailment: 17 / 317 incorrect\n",
      "contradiction: 244 / 996 incorrect\n",
      "\n",
      "2 cue words:\n",
      "entailment: 7 / 107 incorrect\n",
      "contradiction: 64 / 242 incorrect\n",
      "\n",
      "3 cue words:\n",
      "entailment: 3 / 11 incorrect\n",
      "contradiction: 6 / 19 incorrect\n",
      "\n",
      "4 cue words:\n",
      "entailment: 1 / 13 incorrect\n",
      "contradiction: 11 / 29 incorrect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  from ipykernel import kernelapp as app\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "def how_many_cue_words(premise):\n",
    "    total_count = 0\n",
    "    for cue_word in ['no ', 'not ', 'never ', 'nor ']:\n",
    "        total_count += premise.count(cue_word)\n",
    "    return(total_count)\n",
    "\n",
    "for num_cue_words in range(5):\n",
    "    print(\"\\n{} cue words:\".format(num_cue_words))\n",
    "    \n",
    "    df_filtered = df[df.apply(lambda x: how_many_cue_words(x[\"premise\"]) == num_cue_words, axis=1)]\n",
    "\n",
    "    for label in [1, 2]:\n",
    "        incorrect_label = 2 if label == 1 else 1\n",
    "        \n",
    "        df_label = df_filtered[df.labels == label]\n",
    "        total = len(df_label)\n",
    "        num_incorrect = len(df_label[df.preds == incorrect_label])\n",
    "        print(\"{}: {} / {} incorrect\".format(\n",
    "            num_to_label[label], num_incorrect, total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
