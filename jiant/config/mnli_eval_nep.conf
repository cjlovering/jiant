// Taken from superglue_bert.conf

// Config settings used for SuperGLUE BERT baseline experiments.

// This imports the defaults, which can be overridden below.
include "defaults.conf"
exp_name = "mnli-eval-nep"

// Data and preprocessing settings
max_seq_len = 510 // Mainly needed for MultiRC, to avoid over-truncating
                  // But not 512 as that is really hard to fit in memory.

// Model settings
input_module = "bert-base-cased"
transformers_output_mode = "top"
pair_attn = 0 // shouldnt be needed but JIC
s2s = {
    attention = none
}
sent_enc = "none"
sep_embs_for_skip = 1
classifier = log_reg // following BERT paper
transfer_paradigm = finetune 
// finetune entire BERT model

// Control-flow stuff
do_pretrain = 0
do_target_task_training = 0
do_full_eval = 1
write_preds = "val,test"
write_strict_glue_format = 0
// TODO: it does not seem to work o.w.

load_eval_checkpoint = "/data/nlp/rjha/jiant/mnli-small/tuning-0/mnli/model_state_target_train_val_86.best.th"
pretrain_tasks = "mnli"
target_tasks = "nep"
use_classifier = "mnli"
