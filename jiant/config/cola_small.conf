// Taken from superglue_bert.conf

// Config settings used for SuperGLUE BERT baseline experiments.

// This imports the defaults, which can be overridden below.
include "defaults.conf"
exp_name = "cola-small"

// Data and preprocessing settings
max_seq_len = 510 // Mainly needed for MultiRC, to avoid over-truncating
                  // But not 512 as that is really hard to fit in memory.

// Model settings
input_module = "bert-base-cased"
transformers_output_mode = "top"
pytorch_transformers_output_mode = "none"  // How to handle the embedding layer of the
                                           // BERT/XLNet model:
                                           // "none" or "top" returns only top-layer activation,
                                           // "cat" returns top-layer concatenated with
                                           //   lexical layer,
                                           // "only" returns only lexical layer,
                                           // "mix" uses ELMo-style scalar mixing (with learned
                                           //   weights) across all layers.
pytorch_transformers_max_layer = -1  // Maximum layer to return from BERT etc. encoder. Layer 0 is
                                     // wordpiece embeddings. pytorch_transformers_embeddings_mode
                                     // will behave as if the is truncated at this layer, so 'top'
                                     //  will return this layer, and 'mix' will return a mix of all
                                     // layers up to and including this layer.
                                     // Set to -1 to use all layers.
                                     // Used for probing experiments.

pair_attn = 0 // shouldnt be needed but JIC
s2s = {
    attention = none
}
sent_enc = "none"
sep_embs_for_skip = 1
classifier = log_reg // following BERT paper
transfer_paradigm = finetune 
// finetune entire BERT model

// Training settings
dropout = 0.1 // following BERT paper
optimizer = bert_adam
batch_size = 4
max_epochs = 3
lr = .00001
min_lr = .0000001
lr_patience = 4
patience = 20
max_vals = 10000

// Control-flow stuff
do_pretrain = 0
do_target_task_training = 1
do_full_eval = 1
write_preds = "val,test"
write_strict_glue_format = 1

target_tasks = "cola"